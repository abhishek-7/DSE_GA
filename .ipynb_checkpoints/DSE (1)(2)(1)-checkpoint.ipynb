{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#importing base learners of Voting Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Importing three component ensembles\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#importing SVC for second-step classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining ml techniques\n",
    "base_learner1 = LogisticRegression(random_state=1)\n",
    "base_learner2 = DecisionTreeClassifier()\n",
    "base_learner3 = GaussianNB()\n",
    "\n",
    "ensemble1 = VotingClassifier(estimators=[('logregression', base_learner1), \n",
    "                                         ('dtree', base_learner2), \n",
    "                                         ('gnb', base_learner3)], \n",
    "                                          voting='soft')\n",
    "ensemble2 = RandomForestClassifier()\n",
    "ensemble3 = AdaBoostClassifier(n_estimators=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicts bug and their probabilities for given datafile and classifier pair\n",
    "\n",
    "def predict_util(datafile, classifier):\n",
    "    ncols = datafile.columns\n",
    "    #extracting relevant columns, software metrics in X, and labels in Y\n",
    "    if classifier == ensemble1:\n",
    "        ncols = ncols[ :-1]\n",
    "        X     = datafile.iloc[ : , :-1]\n",
    "        X1    = datafile.as_matrix(ncols)\n",
    "    elif classifier == ensemble2:\n",
    "        ncols = ncols[ :-3]\n",
    "        X     = datafile.iloc[ : , :-3]\n",
    "        X1    = datafile.as_matrix(ncols)\n",
    "    else:\n",
    "        ncols = ncols[ :-5]\n",
    "        X     = datafile.iloc[ : , :-5]\n",
    "        X1    = datafile.as_matrix(ncols)\n",
    "    y = datafile['bug_binarized']\n",
    "    Y = np.array(y)\n",
    "    \n",
    "    #performing leave-one out validation for instances less than 100\n",
    "    #and 10 fold validation for others\n",
    "    npoints = X.shape[0]\n",
    "   \n",
    "    if npoints <= 100:\n",
    "        kf = KFold(n_splits = npoints)\n",
    "    else:\n",
    "        kf = KFold(n_splits = 10)\n",
    "        \n",
    "    kf.get_n_splits(X)\n",
    "    train_X = []\n",
    "    train_Y  = []\n",
    "    prediction   = []\n",
    "    predict_prob = [] \n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "\n",
    "        if classifier == ensemble1:\n",
    "            classifier = VotingClassifier(estimators=[('logregression', base_learner1), \n",
    "                                         ('dtree', base_learner2), \n",
    "                                         ('gnb', base_learner3)], \n",
    "                                          voting='soft')      \n",
    "        elif classifier == ensemble2:\n",
    "            classifier = RandomForestClassifier()\n",
    "        else:\n",
    "            classifier = AdaBoostClassifier(n_estimators=50)\n",
    "            \n",
    "\n",
    "        for i in train_index:\n",
    "                train_X.append(X1[i])\n",
    "                train_Y.append(Y[i])\n",
    "        \n",
    "        classifier.fit(train_X, train_Y)\n",
    "        \n",
    "        for j in test_index:\n",
    "            prediction.append(classifier.predict([X1[j]])[0])\n",
    "            predict_prob.append(classifier.predict_proba([X1[j]])[0][1])\n",
    "        \n",
    "        train_X  = []\n",
    "        train_Y  = []\n",
    "        \n",
    "    return prediction, Y, predict_prob\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file=pd.read_csv('dataset/dataset/camel-1.6.csv' , dtype={'bug_binarized':np.bool})\n",
    "#predict_util(file, ensemble3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePerformanceMeasures(predictions, labels, prediction_probability):\n",
    "    \n",
    "    precision = precision_score(y_true=labels, y_pred=predictions)\n",
    "    recall    = recall_score(y_true=labels, y_pred=predictions)\n",
    "    roc_score = roc_auc_score(labels, prediction_probability)\n",
    "    accuracy  = accuracy_score(y_true=labels, y_pred=predictions)\n",
    "    f_measure = 2*(precision*recall)/float(precision+recall) \n",
    "    \n",
    "    metrics = [precision, recall, roc_score, accuracy, f_measure]\n",
    "    \n",
    "    return metrics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    directory = 'dataset/dataset/'\n",
    "    \n",
    "    for projectName in os.listdir(directory):\n",
    "        performanceMetrics = []\n",
    "    \n",
    "        projectData = pd.read_csv(directory + projectName, dtype={'bug_binarized':np.bool})\n",
    "        \n",
    "        metricsFrame = pd.DataFrame(performanceMetrics, \n",
    "                                    index = ['Precision', 'Recall', 'Auc_Score', 'Accuracy', 'F_Measure'])\n",
    "        \n",
    "        predictionEnsemble1, YEnsemble1, predict_probEnsemble1 = predict_util(projectData, ensemble1)\n",
    "        projectData['Voting_Prediction'] = predictionEnsemble1\n",
    "        projectData['Voting_Pred_Prob']  = predict_probEnsemble1\n",
    "        VotingMetrics = computePerformanceMeasures(predictionEnsemble1, YEnsemble1, predict_probEnsemble1)\n",
    "\n",
    "        metricsFrame.insert(loc = 0, column = 'Voting', value = VotingMetrics)\n",
    "                \n",
    "        predictionEnsemble2, YEnsemble2, predict_probEnsemble2 = predict_util(projectData, ensemble2)\n",
    "        projectData['RandomForest_Prediction'] = predictionEnsemble2\n",
    "        projectData['RandomForest_Pred_Prob']  = predict_probEnsemble2\n",
    "        RandomForestMetrics = computePerformanceMeasures(predictionEnsemble2, YEnsemble2, predict_probEnsemble2)\n",
    "        metricsFrame.insert(loc=1, column='RandomForest', value = RandomForestMetrics)\n",
    "        \n",
    "        predictionEnsemble3, YEnsemble3, predict_probEnsemble3 = predict_util(projectData, ensemble3)\n",
    "        projectData['AdaBoost_Prediction'] = predictionEnsemble3\n",
    "        projectData['AdaBoost_Pred_Prob']  = predict_probEnsemble3\n",
    "        AdaBoostMetrics = computePerformanceMeasures(predictionEnsemble2, YEnsemble2, predict_probEnsemble2)\n",
    "        metricsFrame.insert(loc=2, column='AdaBoost', value = AdaBoostMetrics)\n",
    "        \n",
    "        metricsFrame.to_csv('dataset/metrics/' + projectName)\n",
    "\n",
    "        projectData.to_csv('dataset/annotated/' + projectName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestEnsembleSelector():\n",
    "    annotated_directory   = 'dataset/annotated/'\n",
    "    performance_directory = 'dataset/metrics/'\n",
    "\n",
    "    for projectName in os.listdir(annotated_directory):\n",
    "        print(projectName)\n",
    "        annotatedData = pd.read_csv(annotated_directory + projectName, dtype={'bug_binarized':np.bool})\n",
    "        metricData    = pd.read_csv(performance_directory + projectName)\n",
    "\n",
    "\n",
    "        predictionMatrix = annotatedData.as_matrix(columns = ['bug_binarized','Voting_Prediction','AdaBoost_Prediction','RandomForest_Prediction'])\n",
    "        print(metricData)\n",
    "        print(metricData.loc[0, 'Voting'])\n",
    "        \n",
    "       # defining constants\n",
    "        f_measure_constant    = 4     # f-measure is at the 4th row\n",
    "        auc_score_constant    = 2     # auc_score is at the 2nd row\n",
    "        voting_constant       = 'Voting'\n",
    "        adaBoost_constant     = 'AdaBoost'\n",
    "        randomForest_constant = 'RandomForest'\n",
    "        \n",
    "        ensemble=[]\n",
    "        \n",
    "        for i in range(len(predictionMatrix)):\n",
    "            if   predictionMatrix[i][0] == predictionMatrix[i][1] and predictionMatrix[i][0] != predictionMatrix[i][2] and predictionMatrix[i][0] != predictionMatrix[i][3]:\n",
    "                ensemble.append('Voting')\n",
    "            elif predictionMatrix[i][0] == predictionMatrix[i][2] and predictionMatrix[i][0] != predictionMatrix[i][1] and predictionMatrix[i][0] != predictionMatrix[i][3]:\n",
    "                ensemble.append('AdaBoost')\n",
    "            elif predictionMatrix[i][0] == predictionMatrix[i][3] and predictionMatrix[i][0] != predictionMatrix[i][1] and predictionMatrix[i][0] != predictionMatrix[i][2]:\n",
    "                ensemble.append('RandomForest')\n",
    "            else:\n",
    "                f_voting       = metricData.loc[f_measure_constant, voting_constant]\n",
    "                f_adaboost     = metricData.loc[f_measure_constant, adaBoost_constant]\n",
    "                f_randomForest = metricData.loc[f_measure_constant, randomForest_constant]\n",
    "                flag=True\n",
    "                if f_voting > f_adaboost and f_voting > f_randomForest:\n",
    "                    ensemble.append('Voting')\n",
    "                    flag=False\n",
    "                elif f_adaboost > f_randomForest and f_randomForest > f_voting:\n",
    "                    ensemble.append('AdaBoost')\n",
    "                    flag=False\n",
    "                else:\n",
    "                    ensemble.append('RandomForest')\n",
    "                    flag=False\n",
    "            if flag == True:\n",
    "                p_voting       = metricData[auc_score_constant][voting_constant]\n",
    "                p_adaBoost     = metricData[auc_score_constant][adaBoost_constant]\n",
    "                p_randomForest = metricData[auc_score_constant][randomForest_constant]\n",
    "\n",
    "                if p_voting > p_adaBoost and p_voting > p_randomForest:\n",
    "                    ensemble.append('Voting')\n",
    "                    flag=False\n",
    "\n",
    "                elif p_adaBoost>p_randomForest and p_adaBoost>p_voting:\n",
    "                    ensemble.append('AdaBoost')\n",
    "                    flag=False\n",
    "\n",
    "                else:\n",
    "                    ensemble.append('RandomForest')\n",
    "                    flag=False\n",
    "            if flag == True:\n",
    "                ensemble.append('Voting')\n",
    "                \n",
    "        annotatedData['selectedEnsemble'] = ensemble\n",
    "        annotatedData.to_csv(annotated_directory + projectName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestEnsembleSelector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePerformanceMeasuresDSE():\n",
    "    annotated_directory = 'dataset/annotated/'\n",
    "    for projectName in os.listdir(annotated_directory):\n",
    "        DSEPrediction = []\n",
    "        predict_prob  = []\n",
    "        annotatedProject = pd.read_csv(annotated_directory + projectName)\n",
    "        annotatedProjectData = annotatedProject.as_matrix(columns=[\n",
    "                                         'Voting_Prediction',\n",
    "                                         'Voting_Pred_Prob',\n",
    "                                         'AdaBoost_Prediction',\n",
    "                                         'AdaBoost_Pred_Prob',\n",
    "                                         'RandomForest_Prediction',\n",
    "                                         'RandomForest_Pred_Prob'])\n",
    "        ensemble = np.array(annotatedProject['PredictedEnsemble'])\n",
    "        \n",
    "        prediction_constant = '_Prediction'\n",
    "        probab_constant     = '_Pred_Prob'\n",
    "        \n",
    "        for i in range(len(ensemble)):\n",
    "            DSEPrediction.append(annotatedProject.loc[i, ensemble[i] + prediction_constant])\n",
    "            predict_prob.append(annotatedProject.loc[i, ensemble[i] + probab_constant])\n",
    "            \n",
    "#         annotatedProject['DSE_Prediction'] = DSEPrediction\n",
    "#         annotatedProject['DSE_Pred_Prob']  = predict_prob\n",
    "        projectMetrics = []\n",
    "        projectMetrics.append(projectName)\n",
    "        projectMetrics.extend(computePerformanceMeasures(DSEPrediction, annotatedProject['bug_binarized'], predict_prob))\n",
    "        print(projectMetrics)\n",
    "#         metrics = [precision, recall, roc_score, accuracy, f_measure]\n",
    "#         DSEmetricsFrame = pd.DataFrame(performanceMetrics, \n",
    "#                                     index = ['Precision', 'Recall', 'Auc_Score', 'Accuracy', 'F_Measure'])\n",
    "                \n",
    "computePerformanceMeasuresDSE()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svctrain():\n",
    "    directory = 'dataset/dataset/'\n",
    "    annotated_directory = 'dataset/annotated/'\n",
    "    for projectName in os.listdir(directory):\n",
    "        print(projectName)\n",
    "        projectData = pd.read_csv(directory + projectName)\n",
    "        annotatedData = pd.read_csv(annotated_directory + projectName)\n",
    "        \n",
    "        #X contains software metrics and Y best ensemble selected\n",
    "        X = np.array(projectData.iloc[ : , :-1])\n",
    "        Y = np.array(annotatedData.iloc[ : , -1])\n",
    "        \n",
    "        npoints = X.shape[0]\n",
    "        \n",
    "        if npoints <= 100:\n",
    "            kf = KFold(n_splits = npoints)\n",
    "        else:\n",
    "            kf = KFold(n_splits = 10)\n",
    "        \n",
    "        kf.get_n_splits(X)\n",
    "        train_X = []\n",
    "        train_Y = []\n",
    "        \n",
    "        predictedEnsemble = []\n",
    "        predict_prob      = []\n",
    "        final_prediction  = []  # this stores the prediction(bugginess) of the best ensemble predicted by SVC\n",
    "        \n",
    "        prediction_constant = '_Prediction'\n",
    "        probab_constant = '_Pred_Prob'\n",
    "        \n",
    "        \n",
    "        for train_index, test_index in kf.split(X):\n",
    "            classifier = SVC(probability = True)\n",
    "            \n",
    "            for i in train_index:\n",
    "                train_X.append(X[i])\n",
    "                train_Y.append(Y[i])\n",
    "            \n",
    "            unique_labels = np.unique(train_Y)\n",
    "            if unique_labels.size == 1:\n",
    "                for j in test_index:\n",
    "                    predictedEnsemble.append(unique_labels[0])\n",
    "                    predict_prob.append(annotatedData.loc[j, unique_labels[0] + probab_constant])\n",
    "                    final_prediction.append(annotatedData.loc[j, unique_labels[0] + prediction_constant])\n",
    "           \n",
    "            else:\n",
    "                classifier.fit(train_X, train_Y)\n",
    "                \n",
    "                for j in test_index:\n",
    "                    predictedBestEnsemble = classifier.predict([X[j]])[0]\n",
    "                    predictedEnsemble.append(predictedBestEnsemble)\n",
    "                    final_prediction.append(annotatedData.loc[j, predictedBestEnsemble + prediction_constant])\n",
    "                    \n",
    "            # total probability of available classifiers, i.e the classifiers reported in unique_labels predicting true\n",
    "                    predict_proba_true = 0\n",
    "                    \n",
    "            # probability of classifiers being predicted\n",
    "                    predict_proba_classifiers = classifier.predict_proba([X[j]])[0]\n",
    "                    k = 0\n",
    "            # class probabilities are always reported in a sorted by name fashion, i.e AdaBoost, RandomForest, Voting \n",
    "            # np.unique also reports labels in a sorted by name fashion\n",
    "                    for classifierName in unique_labels:\n",
    "                        predict_proba_true +=  predict_proba_classifiers[k] * annotatedData.loc[j, classifierName + probab_constant]\n",
    "                        k += 1\n",
    "                    predict_prob.append(predict_proba_true)\n",
    "                    \n",
    "        annotatedData['PredictedEnsemble'] = predictedEnsemble\n",
    "        annotatedData['DSE_Prediction'] = final_prediction\n",
    "        annotatedData['DSE_Pred_Prob'] = predict_prob\n",
    "        annotatedData.to_csv(annotated_directory + projectName)    \n",
    "#         print(predictedEnsemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svctrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
