{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from operator import itemgetter\n",
    "\n",
    "#importing base learners of Voting Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Importing three component ensembles\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "#importing SVC for second-step classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining ml techniques\n",
    "base_learner1 = LogisticRegression(random_state=1)\n",
    "base_learner2 = DecisionTreeClassifier()\n",
    "base_learner3 = GaussianNB()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicts bug and their probabilities for given datafile and classifier pair\n",
    "\n",
    "def predict_util(datafile, classifiertype):\n",
    "    ncols = datafile.columns\n",
    "    #extracting relevant columns, software metrics in X, and labels in Y\n",
    "    \n",
    "    ncols = ncols[ :-1]\n",
    "    X     = datafile.iloc[ : , :-1]\n",
    "    X1    = datafile.as_matrix(ncols)\n",
    "    y = datafile['buggy']\n",
    "    Y = np.array(y)\n",
    "    \n",
    "    #performing leave-one out validation for instances less than 100\n",
    "    #and 10 fold validation for others\n",
    "    npoints = X.shape[0]\n",
    "   \n",
    "    if npoints <= 100:\n",
    "        kf = KFold(n_splits = npoints)\n",
    "    else:\n",
    "        kf = KFold(n_splits = 10)\n",
    "        \n",
    "    kf.get_n_splits(X)\n",
    "    train_X = []\n",
    "    train_Y  = []\n",
    "    prediction   = []\n",
    "    predict_prob = [] \n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        if classifiertype == 'Voting':\n",
    "            classifier = VotingClassifier(estimators=[\n",
    "                                         ('logregression', base_learner1), \n",
    "                                         ('dtree', base_learner2), \n",
    "                                         ('gnb', base_learner3)], \n",
    "                                          voting='soft')      \n",
    "        elif classifiertype == 'RandomForest':\n",
    "            classifier = RandomForestClassifier()\n",
    "        else:\n",
    "            classifier = AdaBoostClassifier(base_estimator = RandomForestClassifier(), n_estimators = 100, learning_rate = 0.5)\n",
    "            \n",
    "        for i in train_index:\n",
    "                train_X.append(X1[i])\n",
    "                train_Y.append(Y[i])\n",
    "        \n",
    "        classifier.fit(train_X, train_Y)\n",
    "        \n",
    "        for j in test_index:\n",
    "            prediction.append(classifier.predict([X1[j]])[0])\n",
    "            predict_prob.append(classifier.predict_proba([X1[j]])[0][1])\n",
    "        \n",
    "        train_X  = []\n",
    "        train_Y  = []\n",
    "    \n",
    "    return prediction, Y, predict_prob\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePerformanceMeasures(predictions, labels, prediction_probability):\n",
    "    \n",
    "    precision = precision_score(y_true = labels, y_pred = predictions)\n",
    "    recall    = recall_score(y_true = labels, y_pred = predictions)\n",
    "    roc_score = roc_auc_score(labels, prediction_probability)\n",
    "    accuracy  = accuracy_score(y_true = labels, y_pred = predictions)\n",
    "    f_measure = 2*(precision * recall)/float(precision + recall) \n",
    "    g_mean = math.sqrt(precision * recall)\n",
    "    \n",
    "    metrics = [precision, recall, roc_score, accuracy, f_measure, g_mean]\n",
    "    \n",
    "    return metrics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    directory = 'dataset/dataset/'\n",
    "    \n",
    "    for projectName in os.listdir(directory):\n",
    "        performanceMetrics = []\n",
    "    \n",
    "        inputData = pd.read_csv(directory + projectName, dtype={'buggy':np.bool})\n",
    "        projectData = pd.read_csv(directory + projectName, dtype={'buggy':np.bool})\n",
    "        \n",
    "        metricsFrame = pd.DataFrame(performanceMetrics, \n",
    "                                    index = ['Precision', 'Recall', 'Auc_Score', 'Accuracy', 'F_Measure', 'GMean'])\n",
    "        \n",
    "        predictionEnsemble1, YEnsemble1, predict_probEnsemble1 = predict_util(inputData, 'Voting')\n",
    "        projectData['Voting_Prediction'] = predictionEnsemble1\n",
    "        projectData['Voting_Pred_Prob']  = predict_probEnsemble1\n",
    "        VotingMetrics = computePerformanceMeasures(predictionEnsemble1, YEnsemble1, predict_probEnsemble1)\n",
    "        metricsFrame.insert(loc = 0, column = 'Voting', value = VotingMetrics)\n",
    "                \n",
    "            \n",
    "        predictionEnsemble2, YEnsemble2, predict_probEnsemble2 = predict_util(inputData, 'RandomForest')\n",
    "        projectData['RandomForest_Prediction'] = predictionEnsemble2\n",
    "        projectData['RandomForest_Pred_Prob']  = predict_probEnsemble2\n",
    "        RandomForestMetrics = computePerformanceMeasures(predictionEnsemble2, YEnsemble2, predict_probEnsemble2)\n",
    "        metricsFrame.insert(loc = 1, column='RandomForest', value = RandomForestMetrics)\n",
    "        \n",
    "        predictionEnsemble3, YEnsemble3, predict_probEnsemble3 = predict_util(inputData, 'AdaBoost')\n",
    "        projectData['AdaBoost_Prediction'] = predictionEnsemble3\n",
    "        projectData['AdaBoost_Pred_Prob']  = predict_probEnsemble3\n",
    "        AdaBoostMetrics = computePerformanceMeasures(predictionEnsemble3, YEnsemble3, predict_probEnsemble3)\n",
    "        metricsFrame.insert(loc = 2, column='AdaBoost', value = AdaBoostMetrics)\n",
    "        \n",
    "        metricsFrame.to_csv('dataset/metrics/' + projectName)\n",
    "        print(projectName)\n",
    "        print(metricsFrame)\n",
    "        \n",
    "        projectData.to_csv('dataset/annotated/' + projectName, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestEnsembleSelector():\n",
    "    annotated_directory   = 'dataset/annotated/'\n",
    "    performance_directory = 'dataset/metrics/'\n",
    "\n",
    "    for projectName in os.listdir(annotated_directory):\n",
    "        print(projectName)\n",
    "        annotatedData = pd.read_csv(annotated_directory + projectName, dtype={'buggy':np.bool})\n",
    "        metricData    = pd.read_csv(performance_directory + projectName)\n",
    "\n",
    "\n",
    "        predictionMatrix = annotatedData.as_matrix(columns = ['buggy','Voting_Prediction','AdaBoost_Prediction','RandomForest_Prediction'])\n",
    "        print(metricData)\n",
    "        \n",
    "       # defining constants\n",
    "        auc_score_constant    = 2     # auc_score is at the 2nd row\n",
    "        voting_constant       = 'Voting'\n",
    "        adaBoost_constant     = 'AdaBoost'\n",
    "        randomForest_constant = 'RandomForest'\n",
    "        \n",
    "        ensemble=[]\n",
    "        \n",
    "        for i in range(len(predictionMatrix)):\n",
    "            if   predictionMatrix[i][0] == predictionMatrix[i][1] and predictionMatrix[i][0] != predictionMatrix[i][2] and predictionMatrix[i][0] != predictionMatrix[i][3]:\n",
    "                ensemble.append('Voting')\n",
    "            elif predictionMatrix[i][0] == predictionMatrix[i][2] and predictionMatrix[i][0] != predictionMatrix[i][1] and predictionMatrix[i][0] != predictionMatrix[i][3]:\n",
    "                ensemble.append('AdaBoost')\n",
    "            elif predictionMatrix[i][0] == predictionMatrix[i][3] and predictionMatrix[i][0] != predictionMatrix[i][1] and predictionMatrix[i][0] != predictionMatrix[i][2]:\n",
    "                ensemble.append('RandomForest')\n",
    "            else:\n",
    "                p_voting       = metricData.loc[auc_score_constant, voting_constant]\n",
    "                p_adaBoost     = metricData.loc[auc_score_constant, adaBoost_constant]\n",
    "                p_randomForest = metricData.loc[auc_score_constant, randomForest_constant]\n",
    "\n",
    "                if p_voting > p_adaBoost and p_voting > p_randomForest:\n",
    "                    ensemble.append('Voting')\n",
    "\n",
    "                elif p_adaBoost>p_randomForest and p_adaBoost > p_voting:\n",
    "                    ensemble.append('AdaBoost')\n",
    "\n",
    "                else:\n",
    "                    ensemble.append('RandomForest')\n",
    "                \n",
    "        annotatedData['selectedEnsemble'] = ensemble\n",
    "        annotatedData.to_csv(annotated_directory + projectName, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camel-1.6.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.629630      0.745614  0.708029\n",
      "1     Recall  0.274194      0.456989  0.521505\n",
      "2  Auc_Score  0.746755      0.780127  0.780633\n",
      "3   Accuracy  0.771468      0.819945  0.821330\n",
      "4  F_Measure  0.382022      0.566667  0.600619\n",
      "5      GMean  0.415500      0.583727  0.607652\n",
      "e-learning.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.800000      0.800000  0.780000\n",
      "1     Recall  0.653061      0.734694  0.795918\n",
      "2  Auc_Score  0.875333      0.923913  0.922915\n",
      "3   Accuracy  0.822695      0.843972  0.851064\n",
      "4  F_Measure  0.719101      0.765957  0.787879\n",
      "5      GMean  0.722806      0.766652  0.787919\n",
      "intercafe.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.700000      0.833333  0.727273\n",
      "1     Recall  0.777778      0.555556  0.888889\n",
      "2  Auc_Score  0.746032      0.928571  0.908730\n",
      "3   Accuracy  0.782609      0.782609  0.826087\n",
      "4  F_Measure  0.736842      0.666667  0.800000\n",
      "5      GMean  0.737865      0.680414  0.804030\n",
      "ivy-2.0.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.770270      0.769231  0.791045\n",
      "1     Recall  0.686747      0.602410  0.638554\n",
      "2  Auc_Score  0.875502      0.901720  0.904604\n",
      "3   Accuracy  0.854730      0.837838  0.851351\n",
      "4  F_Measure  0.726115      0.675676  0.706667\n",
      "5      GMean  0.727311      0.680729  0.710721\n",
      "jedit-4.3.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.760331      0.919463  0.921569\n",
      "1     Recall  0.630137      0.938356  0.965753\n",
      "2  Auc_Score  0.895205      0.970228  0.967565\n",
      "3   Accuracy  0.817982      0.953947  0.962719\n",
      "4  F_Measure  0.689139      0.928814  0.943144\n",
      "5      GMean  0.692179      0.928862  0.943402\n",
      "kalkulator.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.615385      0.777778  0.727273\n",
      "1     Recall  0.800000      0.700000  0.800000\n",
      "2  Auc_Score  0.885714      0.828571  0.800000\n",
      "3   Accuracy  0.708333      0.791667  0.791667\n",
      "4  F_Measure  0.695652      0.736842  0.761905\n",
      "5      GMean  0.701646      0.737865  0.762770\n",
      "log4j-1.2.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.911504      0.927273  0.926230\n",
      "1     Recall  0.865546      0.857143  0.949580\n",
      "2  Auc_Score  0.939694      0.946762  0.975334\n",
      "3   Accuracy  0.872549      0.877451  0.926471\n",
      "4  F_Measure  0.887931      0.890830  0.937759\n",
      "5      GMean  0.888228      0.891518  0.937832\n",
      "lucene-2.4.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.666667      0.642857  0.656863\n",
      "1     Recall  0.641509      0.594340  0.632075\n",
      "2  Auc_Score  0.703815      0.663643  0.686917\n",
      "3   Accuracy  0.634518      0.604061  0.624365\n",
      "4  F_Measure  0.653846      0.617647  0.644231\n",
      "5      GMean  0.653967      0.618123  0.644350\n",
      "poi-2.5.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.909639      0.903614  0.881356\n",
      "1     Recall  0.857955      0.852273  0.886364\n",
      "2  Auc_Score  0.902379      0.906406  0.924357\n",
      "3   Accuracy  0.850187      0.842697  0.846442\n",
      "4  F_Measure  0.883041      0.877193  0.883853\n",
      "5      GMean  0.883419      0.877568  0.883856\n",
      "prop-6.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.544304      0.658683  0.678571\n",
      "1     Recall  0.502924      0.643275  0.777778\n",
      "2  Auc_Score  0.817173      0.858030  0.898810\n",
      "3   Accuracy  0.746365      0.809370  0.836834\n",
      "4  F_Measure  0.522796      0.650888  0.724796\n",
      "5      GMean  0.523205      0.650933  0.726483\n",
      "redaktor.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.897959      0.933333  0.913043\n",
      "1     Recall  0.862745      0.823529  0.823529\n",
      "2  Auc_Score  0.924683      0.931257  0.930681\n",
      "3   Accuracy  0.911765      0.911765  0.904412\n",
      "4  F_Measure  0.880000      0.875000  0.865979\n",
      "5      GMean  0.880176      0.876714  0.867132\n",
      "serapion.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.571429      0.571429  0.571429\n",
      "1     Recall  0.571429      0.571429  0.571429\n",
      "2  Auc_Score  0.777143      0.820000  0.822857\n",
      "3   Accuracy  0.812500      0.812500  0.812500\n",
      "4  F_Measure  0.571429      0.571429  0.571429\n",
      "5      GMean  0.571429      0.571429  0.571429\n",
      "sklebagd.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.857143      0.833333  0.833333\n",
      "1     Recall  0.857143      0.714286  0.714286\n",
      "2  Auc_Score  0.809524      0.714286  0.761905\n",
      "3   Accuracy  0.800000      0.700000  0.700000\n",
      "4  F_Measure  0.857143      0.769231  0.769231\n",
      "5      GMean  0.857143      0.771517  0.771517\n",
      "synapse-1.2.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.666667      0.600000  0.450000\n",
      "1     Recall  0.347826      0.391304  0.391304\n",
      "2  Auc_Score  0.783421      0.674319  0.761276\n",
      "3   Accuracy  0.853846      0.846154  0.807692\n",
      "4  F_Measure  0.457143      0.473684  0.418605\n",
      "5      GMean  0.481543      0.484544  0.419627\n",
      "systemdata.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.600000      0.500000  0.428571\n",
      "1     Recall  0.428571      0.428571  0.428571\n",
      "2  Auc_Score  0.835498      0.746753  0.653680\n",
      "3   Accuracy  0.850000      0.825000  0.800000\n",
      "4  F_Measure  0.500000      0.461538  0.428571\n",
      "5      GMean  0.507093      0.462910  0.428571\n",
      "szybkafucha.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.750000      0.727273  0.727273\n",
      "1     Recall  0.900000      0.800000  0.800000\n",
      "2  Auc_Score  0.762500      0.806250  0.818750\n",
      "3   Accuracy  0.777778      0.722222  0.722222\n",
      "4  F_Measure  0.818182      0.761905  0.761905\n",
      "5      GMean  0.821584      0.762770  0.762770\n",
      "termoproject.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.600000      0.400000  0.400000\n",
      "1     Recall  0.375000      0.250000  0.250000\n",
      "2  Auc_Score  0.846591      0.761364  0.764205\n",
      "3   Accuracy  0.766667      0.700000  0.700000\n",
      "4  F_Measure  0.461538      0.307692  0.307692\n",
      "5      GMean  0.474342      0.316228  0.316228\n",
      "tomcat.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.727273      0.857143  0.897727\n",
      "1     Recall  0.448598      0.560748  0.738318\n",
      "2  Auc_Score  0.856014      0.920668  0.930317\n",
      "3   Accuracy  0.877971      0.909667  0.941363\n",
      "4  F_Measure  0.554913      0.677966  0.810256\n",
      "5      GMean  0.571186      0.693283  0.814130\n",
      "velocity-1.6.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.647059      0.592593  0.694444\n",
      "1     Recall  0.511628      0.372093  0.581395\n",
      "2  Auc_Score  0.826404      0.730327  0.812309\n",
      "3   Accuracy  0.767606      0.732394  0.795775\n",
      "4  F_Measure  0.571429      0.457143  0.632911\n",
      "5      GMean  0.575372      0.469574  0.635411\n",
      "workflow.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.777778      0.700000  0.700000\n",
      "1     Recall  0.777778      0.777778  0.777778\n",
      "2  Auc_Score  0.675214      0.658120  0.623932\n",
      "3   Accuracy  0.818182      0.772727  0.772727\n",
      "4  F_Measure  0.777778      0.736842  0.736842\n",
      "5      GMean  0.777778      0.737865  0.737865\n",
      "wspomaganiepi.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.888889      0.875000  0.875000\n",
      "1     Recall  0.888889      0.777778  0.777778\n",
      "2  Auc_Score  0.861111      0.916667  0.944444\n",
      "3   Accuracy  0.846154      0.769231  0.769231\n",
      "4  F_Measure  0.888889      0.823529  0.823529\n",
      "5      GMean  0.888889      0.824958  0.824958\n",
      "xalan-2.7.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.962585      0.985965  0.986111\n",
      "1     Recall  0.986063      0.979094  0.989547\n",
      "2  Auc_Score  0.998055      0.997649  0.997837\n",
      "3   Accuracy  0.975884      0.983923  0.988746\n",
      "4  F_Measure  0.974182      0.982517  0.987826\n",
      "5      GMean  0.974253      0.982523  0.987828\n",
      "xerces-1.4.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.927536      0.902778  0.907143\n",
      "1     Recall  0.927536      0.942029  0.920290\n",
      "2  Auc_Score  0.914068      0.922705  0.905578\n",
      "3   Accuracy  0.915612      0.907173  0.898734\n",
      "4  F_Measure  0.927536      0.921986  0.913669\n",
      "5      GMean  0.927536      0.922195  0.913693\n",
      "zuzel.csv\n",
      "  Unnamed: 0    Voting  RandomForest  AdaBoost\n",
      "0  Precision  0.875000      0.777778  0.777778\n",
      "1     Recall  0.777778      0.777778  0.777778\n",
      "2  Auc_Score  0.888889      0.916667  0.902778\n",
      "3   Accuracy  0.823529      0.764706  0.764706\n",
      "4  F_Measure  0.823529      0.777778  0.777778\n",
      "5      GMean  0.824958      0.777778  0.777778\n"
     ]
    }
   ],
   "source": [
    "bestEnsembleSelector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svctrain():\n",
    "    directory = 'dataset/dataset/'\n",
    "    annotated_directory = 'dataset/annotated/'\n",
    "    DSE_directory = 'dataset/DSE/'\n",
    "    for projectName in os.listdir(directory):\n",
    "        print(projectName)\n",
    "        projectData = pd.read_csv(directory + projectName)\n",
    "        annotatedData = pd.read_csv(annotated_directory + projectName)\n",
    "        \n",
    "        #X contains software metrics and Y best ensemble selected\n",
    "        X = np.array(projectData.iloc[ : , :-1])\n",
    "        Y = np.array(annotatedData.iloc[ : , -1])\n",
    "        \n",
    "        npoints = X.shape[0]\n",
    "        \n",
    "        if npoints <= 100:\n",
    "            kf = KFold(n_splits = npoints)\n",
    "        else:\n",
    "            kf = KFold(n_splits = 10)\n",
    "        \n",
    "        kf.get_n_splits(X)\n",
    "        train_X = []\n",
    "        train_Y = []\n",
    "        \n",
    "        predictedEnsemble = []\n",
    "        predict_prob      = []\n",
    "        final_prediction  = []  # this stores the prediction(bugginess) of the best ensemble predicted by SVC\n",
    "        \n",
    "        prediction_constant = '_Prediction'\n",
    "        probab_constant = '_Pred_Prob'\n",
    "        \n",
    "        \n",
    "        for train_index, test_index in kf.split(X):\n",
    "            classifier = SVC(probability = True)\n",
    "            \n",
    "            for i in train_index:\n",
    "                train_X.append(X[i])\n",
    "                train_Y.append(Y[i])\n",
    "            \n",
    "            unique_labels = np.unique(train_Y)\n",
    "            if unique_labels.size == 1:\n",
    "                for j in test_index:\n",
    "                    predictedEnsemble.append(unique_labels[0])\n",
    "                    predict_prob.append(annotatedData.loc[j, unique_labels[0] + probab_constant])\n",
    "                    final_prediction.append(annotatedData.loc[j, unique_labels[0] + prediction_constant])\n",
    "           \n",
    "            else:\n",
    "                classifier.fit(train_X, train_Y)\n",
    "                \n",
    "                for j in test_index:\n",
    "                    predictedBestEnsemble = classifier.predict([X[j]])[0]\n",
    "                    predictedEnsemble.append(predictedBestEnsemble)\n",
    "                    final_prediction.append(annotatedData.loc[j, predictedBestEnsemble + prediction_constant])\n",
    "                    \n",
    "            # total probability of available classifiers, i.e the classifiers reported in unique_labels predicting true\n",
    "                    predict_proba_true = 0\n",
    "                    \n",
    "            # probability of classifiers being predicted\n",
    "                    predict_proba_classifiers = classifier.predict_proba([X[j]])[0]\n",
    "                    k = 0\n",
    "            # class probabilities are always reported in a sorted by name fashion, i.e AdaBoost, RandomForest, Voting \n",
    "            # np.unique also reports labels in a sorted by name fashion\n",
    "                    for classifierName in unique_labels:\n",
    "                        predict_proba_true +=  predict_proba_classifiers[k] * annotatedData.loc[j, classifierName + probab_constant]\n",
    "                        k += 1\n",
    "                    predict_prob.append(predict_proba_true)\n",
    "                    \n",
    "        annotatedData['PredictedEnsemble'] = predictedEnsemble\n",
    "        annotatedData['DSE_Prediction'] = final_prediction\n",
    "        annotatedData['DSE_Pred_Prob'] = predict_prob\n",
    "        annotatedData.to_csv(DSE_directory + projectName, index = False)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svctrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePerformanceMeasuresDSE():\n",
    "    DSEdirectory = 'dataset/DSE/'\n",
    "    projectMetrics = []\n",
    "    index = 0\n",
    "    projectMetrics = pd.DataFrame(projectMetrics,\n",
    "                                    columns = ['Project','Precision', 'Recall', 'Auc_Score', 'Accuracy', 'Fmeasure', 'GMean'])\n",
    "    for projectName in os.listdir(DSEdirectory):\n",
    "        project = pd.read_csv(DSEdirectory + projectName)\n",
    "        projectData = project.as_matrix(columns=[\n",
    "                                         'DSE_Prediction',\n",
    "                                         'DSE_Pred_Prob',\n",
    "                                         'buggy'])\n",
    "      \n",
    "        row = []\n",
    "        row.append(projectName)\n",
    "        row.extend(computePerformanceMeasures(project['DSE_Prediction'], \n",
    "                                                         project['buggy'], \n",
    "                                                         project['DSE_Pred_Prob']))\n",
    "        projectMetrics.loc[index] = row\n",
    "        index = index + 1\n",
    "  \n",
    "    print(projectMetrics)\n",
    "    projectMetrics.to_csv('dataset/' + 'results.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Project  Precision    Recall  Auc_Score  Accuracy  Fmeasure  \\\n",
      "0       camel-1.6.csv   0.700000  0.526882   0.784410  0.819945  0.601227   \n",
      "1      e-learning.csv   0.897959  0.897959   0.945874  0.929078  0.897959   \n",
      "2       intercafe.csv   0.833333  0.555556   0.928571  0.782609  0.666667   \n",
      "3         ivy-2.0.csv   0.894737  0.819277   0.939024  0.922297  0.855346   \n",
      "4       jedit-4.3.csv   0.926667  0.952055   0.971410  0.960526  0.939189   \n",
      "5      kalkulator.csv   0.666667  0.800000   0.885714  0.750000  0.727273   \n",
      "6       log4j-1.2.csv   0.974359  0.957983   0.989521  0.960784  0.966102   \n",
      "7      lucene-2.4.csv   0.699029  0.679245   0.735538  0.670051  0.688995   \n",
      "8         poi-2.5.csv   0.893258  0.903409   0.922172  0.865169  0.898305   \n",
      "9          prop-6.csv   0.803571  0.789474   0.916510  0.888530  0.796460   \n",
      "10       redaktor.csv   0.902439  0.725490   0.931373  0.867647  0.804348   \n",
      "11       serapion.csv   0.571429  0.571429   0.825714  0.812500  0.571429   \n",
      "12       sklebagd.csv   0.857143  0.857143   0.809524  0.800000  0.857143   \n",
      "13    synapse-1.2.csv   0.785714  0.478261   0.791142  0.884615  0.594595   \n",
      "14     systemdata.csv   0.600000  0.428571   0.835498  0.850000  0.500000   \n",
      "15    szybkafucha.csv   0.750000  0.900000   0.837500  0.777778  0.818182   \n",
      "16   termoproject.csv   0.571429  0.500000   0.846591  0.766667  0.533333   \n",
      "17         tomcat.csv   0.879518  0.682243   0.934793  0.930269  0.768421   \n",
      "18   velocity-1.6.csv   0.696970  0.534884   0.852478  0.788732  0.605263   \n",
      "19       workflow.csv   0.700000  0.777778   0.700855  0.772727  0.736842   \n",
      "20  wspomaganiepi.csv   1.000000  0.888889   0.944444  0.923077  0.941176   \n",
      "21      xalan-2.7.csv   0.965870  0.986063   0.998211  0.977492  0.975862   \n",
      "22     xerces-1.4.csv   0.920290  0.920290   0.916776  0.907173  0.920290   \n",
      "23          zuzel.csv   0.875000  0.777778   0.916667  0.823529  0.823529   \n",
      "\n",
      "       GMean  \n",
      "0   0.607303  \n",
      "1   0.897959  \n",
      "2   0.680414  \n",
      "3   0.856176  \n",
      "4   0.939275  \n",
      "5   0.730297  \n",
      "6   0.966136  \n",
      "7   0.689066  \n",
      "8   0.898319  \n",
      "9   0.796491  \n",
      "10  0.809142  \n",
      "11  0.571429  \n",
      "12  0.857143  \n",
      "13  0.613006  \n",
      "14  0.507093  \n",
      "15  0.821584  \n",
      "16  0.534522  \n",
      "17  0.774626  \n",
      "18  0.610572  \n",
      "19  0.737865  \n",
      "20  0.942809  \n",
      "21  0.975914  \n",
      "22  0.920290  \n",
      "23  0.824958  \n"
     ]
    }
   ],
   "source": [
    "computePerformanceMeasuresDSE()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutationRate = 0.001\n",
    "crossOverRate = 0.06\n",
    "iterations = 10\n",
    "poolSize = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roulette(fitness):\n",
    "    index = 0\n",
    "    cumalativeFitness = 0.0\n",
    "    r = random.random()\n",
    "    \n",
    "    for i in range(len(fitness)):\n",
    "        cumalativeFitness += fitness[i]\n",
    "        if cumalativeFitness > r:\n",
    "            return i\n",
    "\n",
    "\n",
    "def selectFittest(fitness, rankedPool):\n",
    "    while True:\n",
    "        idx1 = roulette(fitness)\n",
    "        idx2 = roulette(fitness)\n",
    "        \n",
    "        if idx1 is None or idx2 is None:\n",
    "            continue\n",
    "        elif idx1==idx2:\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return rankedPool[idx1], rankedPool[idx2]\n",
    "\n",
    "def crossover(chrome1, chrome2):\n",
    "    randomSplitPoint = random.randint(1, len(chrome1))\n",
    "    print randomSplitPoint\n",
    "    print len(chrome1),len(chrome2)\n",
    "    return chrome1[:randomSplitPoint]+chrome2[randomSplitPoint:], chrome2[:randomSplitPoint]+chrome1[randomSplitPoint:]\n",
    "\n",
    "\n",
    "def mutate(chromosome):\n",
    "#     print chromosome\n",
    "    mutatedChrom = []\n",
    "    for ch in chromosome:\n",
    "        if random.random()<mutationRate:\n",
    "            if ch==1:\n",
    "                mutatedChrom.append(0)\n",
    "            else:\n",
    "                mutatedChrom.append(1)\n",
    "        else:\n",
    "            mutatedChrom.append(ch)\n",
    "    return mutatedChrom\n",
    "    \n",
    "def breed(chrome1, chrome2):\n",
    "    if random.random()<crossOverRate:\n",
    "        newChrome1, newChrome2 = crossover(chrome1, chrome2)\n",
    "    else:\n",
    "        newChrome1 = chrome1\n",
    "        newChrome2 = chrome2\n",
    "        \n",
    "    newChrome1 = mutate(newChrome1)\n",
    "    newChrome2 = mutate(newChrome2)\n",
    "    \n",
    "    return newChrome1, newChrome2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankPop(pool, X, y, classifier):\n",
    "    scores = []\n",
    "    \n",
    "    for chromosome in pool:\n",
    "#         print chromosome\n",
    "        chosen_idx = [idx for gene, idx in zip(chromosome, range(X.shape[1])) if gene==1]\n",
    "        if len(chosen_idx)==0:\n",
    "            continue\n",
    "        chosenX = X.iloc[:, chosen_idx]\n",
    "        \n",
    "        classifier.fit(chosenX, y)\n",
    "        scores.append(accuracy_score(y, classifier.predict(chosenX)))\n",
    "        \n",
    "    fitness = [x/sum(scores) for x in scores]\n",
    "    pairedPop = zip(pool, fitness)\n",
    "    rankedPop = sorted(pairedPop, key=itemgetter(-1), reverse = True)\n",
    "    \n",
    "    return rankedPop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteratePop(rankedPop):\n",
    "    fitness = [item[-1] for item in rankedPop]\n",
    "    rankedPool = [item[0] for item in rankedPop]\n",
    "   \n",
    "    new_pool = []\n",
    "    new_pool.extend(rankedPool[:poolSize/15])\n",
    "    \n",
    "    while(len(new_pool)<poolSize):\n",
    "        ch1, ch2 = selectFittest(fitness, rankedPool)\n",
    "        ch1, ch2 = breed(ch1, ch2)\n",
    "        \n",
    "        new_pool.append(ch1)\n",
    "        new_pool.append(ch2)\n",
    "    \n",
    "    return new_pool[:poolSize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgoFit(datafile):\n",
    "    datafile = pd.read_csv(datafile, dtype={'buggy':np.bool})\n",
    "    print datafile.head()\n",
    "    X     = datafile.iloc[ : , :-1]\n",
    "    y = datafile['buggy']\n",
    "    \n",
    "    classifier = RandomForestClassifier()\n",
    "  \n",
    "    pool = np.random.randint(0, 2, (poolSize, X.shape[1]))  \n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        rankedPop = rankPop(pool, X, y, classifier)\n",
    "        pool = []\n",
    "        pool = iteratePop(rankedPop)\n",
    "        \n",
    "    best_chromosome = rankPop(pool, X, y, classifier)[0][0]\n",
    "    return best_chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   wmc  dit  noc  cbo  rfc  lcom  ca  ce  npm     lcom3  ...    dam  moa  \\\n",
      "0    5    3    0    7   10     0   1   7    4  0.250000  ...    1.0    1   \n",
      "1    4    1    0    3    5     4   1   2    3  0.666667  ...    1.0    1   \n",
      "2   20    4    0   26   95   144   2  26   13  0.842105  ...    1.0    0   \n",
      "3    3    2    0    8   22     3   2   6    2  2.000000  ...    0.0    0   \n",
      "4    8    1    0   25   20    22  22   3    6  0.571429  ...    1.0    0   \n",
      "\n",
      "        mfa       cam  ic  cbm     amc  max_cc  avg_cc  buggy  \n",
      "0  0.921053  0.360000   1    2   7.400       1  0.6000  False  \n",
      "1  0.000000  0.500000   0    0   3.000       1  0.5000  False  \n",
      "2  0.727273  0.197368   4    5  20.300       3  1.0000  False  \n",
      "3  0.750000  0.666667   1    3  54.000      15  5.3333   True  \n",
      "4  0.000000  0.250000   0    0  20.875       1  0.7500   True  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "1\n",
      "20 20\n",
      "1\n",
      "20 20\n",
      "4\n",
      "20 20\n",
      "19\n",
      "20 20\n",
      "11\n",
      "20 20\n",
      "11\n",
      "20 20\n",
      "2\n",
      "20 20\n",
      "9\n",
      "20 20\n",
      "15\n",
      "20 20\n",
      "1\n",
      "20 20\n",
      "11\n",
      "20 20\n",
      "14\n",
      "20 20\n",
      "11\n",
      "20 20\n",
      "3\n",
      "20 20\n",
      "9\n",
      "20 20\n",
      "3\n",
      "20 20\n",
      "8\n",
      "20 20\n",
      "10\n",
      "20 20\n",
      "13\n",
      "20 20\n",
      "10\n",
      "20 20\n",
      "6\n",
      "20 20\n",
      "[0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "datafile = 'dataset/dataset/camel-1.6.csv'\n",
    "print geneticAlgoFit(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. IN pred_util function instead of directly training the data train on selected featured by geneticAlgoFit\n",
    "2. Set the parameters for GA.\n",
    "3. Remaining process remains same having 3 ensembles and 1 fitness function\n",
    "4. To be used without WEKA\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafile = 'dataset/dataset/camel-1.6.csv'\n",
    "len(pd.read_csv(datafile).columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
